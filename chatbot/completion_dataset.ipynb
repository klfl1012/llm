{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "writing500 = load_dataset(\"alvinming/writing_500\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "writing500 = writing500.map(\n",
    "    lambda example: {\"question\": example[\"question\"].replace(\"\\n\", \" \")},\n",
    "    batched=False,\n",
    "    remove_columns=[\"qid\",\"citation_numbers\", \"faithful_answer_w_citation\", \"gold_doc_ids\", \"contexts\", \"unfaithful_answer\", \"unfaithful_justification\"]\n",
    ")\n",
    "\n",
    "writing500 = writing500.rename_columns({\"question\": \"inputs\", \"answer\": \"labels\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': 'is it appropriate to use the salutation dear all in a work email?',\n",
       " 'labels': 'Using \"Dear all\" is perfectly acceptable for addressing a group, as is \"Dear Colleagues\"; the choice between them really hinges on the desired level of formality and the common practices within your particular work environment.'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writing500[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrections = load_dataset(\"jhu-clsp/jfleg\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "corrections = corrections.map(\n",
    "    lambda example: {\"corrections\": random.choice(example[\"corrections\"])},\n",
    "    batched=False,\n",
    ")\n",
    "\n",
    "corrections = corrections.rename_columns({\"corrections\": \"labels\", \"sentence\": \"inputs\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': 'So I think we can not live if old people could not find siences and tecnologies and they did not developped . ',\n",
       " 'labels': 'So I think we would not be alive if our ancestors did not develop sciences and technologies . '}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrections[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 220748/220748 [00:00<00:00, 354967.28 examples/s]\n",
      "Generating validation split: 100%|██████████| 11392/11392 [00:00<00:00, 340183.62 examples/s]\n",
      "Generating test split: 100%|██████████| 10695/10695 [00:00<00:00, 315393.35 examples/s]\n",
      "Generating challenge_train_sample split: 100%|██████████| 500/500 [00:00<00:00, 99560.96 examples/s]\n",
      "Generating challenge_validation_sample split: 100%|██████████| 500/500 [00:00<00:00, 89411.72 examples/s]\n",
      "Generating challenge_test_covid split: 100%|██████████| 5058/5058 [00:00<00:00, 297350.79 examples/s]\n"
     ]
    }
   ],
   "source": [
    "mlsum = load_dataset(\"GEM/mlsum\", \"de\", split=\"validation\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 11392/11392 [00:00<00:00, 30282.96 examples/s]\n"
     ]
    }
   ],
   "source": [
    "mlsum = mlsum.map(\n",
    "    batched=False,\n",
    "    remove_columns=['gem_id', 'gem_parent_id', 'topic', 'url', 'title', 'date', 'references'],\n",
    ")\n",
    "\n",
    "mlsum = mlsum.rename_columns({\"text\": \"inputs\", \"target\": \"labels\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 11392/11392 [00:00<00:00, 114510.91 examples/s]\n"
     ]
    }
   ],
   "source": [
    "mlsum = mlsum.filter(lambda example: len(example[\"inputs\"]) < 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = max(len(text) for text in mlsum[\"inputs\"])\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "xsum = load_dataset(\"EdinburghNLP/xsum\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 11332/11332 [00:00<00:00, 48375.01 examples/s]\n",
      "Filter: 100%|██████████| 11332/11332 [00:00<00:00, 348556.44 examples/s]\n"
     ]
    }
   ],
   "source": [
    "xsum = xsum.map(\n",
    "    lambda example: {\"inputs\": example[\"document\"].replace(\"\\n\", \" \"), \"labels\": example[\"summary\"]},  \n",
    "    batched=False,\n",
    "    remove_columns=[\"id\", \"summary\", \"document\"],\n",
    ")\n",
    "\n",
    "xsum = xsum.filter(lambda example: len(example[\"inputs\"]) < 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': 'The ex-Reading defender denied fraudulent trading charges relating to the Sodje Sports Foundation - a charity to raise money for Nigerian sport. Mr Sodje, 37, is jointly charged with elder brothers Efe, 44, Bright, 50 and Stephen, 42. Appearing at the Old Bailey earlier, all four denied the offence. The charge relates to offences which allegedly took place between 2008 and 2014. Sam, from Kent, Efe and Bright, of Greater Manchester, and Stephen, from Bexley, are due to stand trial in July. They were all released on bail.',\n",
       " 'labels': 'Former Premier League footballer Sam Sodje has appeared in court alongside three brothers accused of charity fraud.'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xsum[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "merged = concatenate_datasets([writing500, corrections, mlsum, xsum])\n",
    "merged = merged.shuffle(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged.filter(lambda example: example[\"inputs\"].strip() != \"\" and example[\"labels\"].strip() != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4458/4458 [00:00<00:00, 27452.07 examples/s]\n"
     ]
    }
   ],
   "source": [
    "merged = merged.map(\n",
    "    lambda example: {\"inputs\": example[\"inputs\"].replace(\"\\'\", \"\") and example[\"inputs\"].replace('\"', \"\")},\n",
    "    batched=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating CSV from Arrow format: 100%|██████████| 5/5 [00:00<00:00, 137.26ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2884989"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.to_csv(\"merged1.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
